{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[ 1.62434536 -0.61175641]\n",
      " [-0.52817175 -1.07296862]]\n",
      "y:\n",
      " [2 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly generate data \n",
    "N = 2 # number of training sample \n",
    "d = 2 # data dimension \n",
    "C = 3 # number of classes \n",
    "\n",
    "X = np.random.randn(d, N)\n",
    "y = np.random.randint(0, 3, (N,))\n",
    "print(\"X:\\n\", X)\n",
    "print(\"y:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      " [2 1]\n",
      "Y:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "(array([1, 1]), (array([2, 1]), array([0, 1])))\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse \n",
    "def convert_labels(y, C = C):\n",
    "    \"\"\"\n",
    "    convert 1d label to a matrix label one-host\n",
    "    \"\"\"\n",
    "    Y = sparse.coo_matrix((np.ones_like(y), \n",
    "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
    "    return Y \n",
    "\n",
    "Y = convert_labels(y, C)\n",
    "print(\"y:\\n\", y)\n",
    "print(\"Y:\\n\", Y)\n",
    "\n",
    "print((np.ones_like(y),(y, np.arange(len(y)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    e_Z = np.exp(Z)\n",
    "    A = e_Z / e_Z.sum(axis = 0)\n",
    "    return A\n",
    "\n",
    "# loss function  \n",
    "def loss(X, Y, W):\n",
    "    A = softmax(W.T.dot(X))\n",
    "    return -np.sum(Y*np.log(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_init:\n",
      "[[ 0.17246684 -1.77382823  1.21958759]\n",
      " [-0.94770704  1.55915476  1.27115498]]\n",
      "C:\n",
      "3\n",
      "Y:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "N:\n",
      "2\n",
      "d:\n",
      "2\n",
      "W final:\n",
      "[[-0.3185071  -3.72815912  3.66489242]\n",
      " [ 2.53472793 -1.77884557  1.12672034]]\n"
     ]
    }
   ],
   "source": [
    "def softmax_regression(X, y, W_init, eta, tol = 1e-4, max_count = 10000):\n",
    "    print(\"W_init:\")\n",
    "    print(W_init)\n",
    "    W = [W_init]    \n",
    "    C = W_init.shape[1]\n",
    "    print(\"C:\")\n",
    "    print(C)\n",
    "    Y = convert_labels(y, C)\n",
    "    print(\"Y:\")\n",
    "    print(Y)\n",
    "    it = 0\n",
    "    N = X.shape[1]\n",
    "    print(\"N:\")\n",
    "    print(N)\n",
    "    d = X.shape[0]\n",
    "    print(\"d:\")\n",
    "    print(d)\n",
    "    \n",
    "    count = 0\n",
    "    check_w_after = 20\n",
    "    while count < max_count:\n",
    "        # mix data \n",
    "        mix_id = np.random.permutation(N)\n",
    "        for i in mix_id:\n",
    "            xi = X[:, i].reshape(d, 1)\n",
    "            yi = Y[:, i].reshape(C, 1)\n",
    "            ai = softmax(np.dot(W[-1].T, xi))\n",
    "            W_new = W[-1] + eta*xi.dot((yi - ai).T)\n",
    "            count += 1\n",
    "            # stopping criteria\n",
    "            if count%check_w_after == 0:                \n",
    "                if np.linalg.norm(W_new - W[-check_w_after]) < tol:\n",
    "                    return W\n",
    "            W.append(W_new)\n",
    "    return W\n",
    "eta = .05 \n",
    "d = X.shape[0]\n",
    "W_init = np.random.randn(d, C)\n",
    "\n",
    "W = softmax_regression(X, y, W_init, eta)\n",
    "print(\"W final:\")\n",
    "print(W[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(W, X):\n",
    "    A = softmax(W.T.dot(X))\n",
    "    return np.argmax(A, axis = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
